import { Message } from '../../hooks/useApi';
import { RAGService } from '../rag/RAGService';
import { ResponseValidator } from '../rag/ResponseValidator';

// --- Shared Interfaces (must match native version) ---

interface GemmaConfig {
  modelPath?: string;
  maxTokens?: number;
  temperature?: number;
}

export interface ChatCompletionParams {
  model: string;
  messages: Message[];
  max_tokens?: number;
  temperature?: number;
  stream?: boolean;
}

export interface ChatCompletionResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: {
    index: number;
    message: {
      role: 'assistant';
      content: string;
    };
    finish_reason: string;
  }[];
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

// --- Web Implementation (Mock/Simulation) ---

export class GemmaClient {
  private config: GemmaConfig;
  private isModelLoaded: boolean = false;
  private ragService: RAGService;
  private validator: ResponseValidator;

  constructor(config: GemmaConfig = {}) {
    this.ragService = RAGService.getInstance();
    this.validator = ResponseValidator.getInstance();
    this.config = {
      modelPath: config.modelPath || 'gemma-web-mock',
      maxTokens: config.maxTokens || 2048,
      temperature: config.temperature || 0.7,
    };
  }

  async loadModel(): Promise<void> {
    console.log('[Web Mock] Loading Gemma model...');
    // Simulate loading time
    await new Promise(resolve => setTimeout(resolve, 500));
    this.isModelLoaded = true;
    console.log('[Web Mock] Gemma model loaded successfully');
  }

  async createChatCompletion(params: ChatCompletionParams): Promise<ChatCompletionResponse> {
    if (!this.isModelLoaded) {
      await this.loadModel();
    }

    const lastUserMessage = params.messages.filter(msg => msg.role === 'user').pop()?.content || '';
    const conversationHistory = this.formatConversationHistory(params.messages);
    
    // Use RAG service to process the query
    const ragContext = await this.ragService.processQuery(lastUserMessage);
    
    let responseContent: string;

    // If RAG finds relevant knowledge, use it
    if (ragContext.relevantKnowledge.length > 0) {
      console.log('[Web Mock] RAG service found relevant knowledge');
      responseContent = this.getFormattedRAGResponse(ragContext);
    } else {
      // For web version, provide a contextual fallback response
      console.log('[Web Mock] No RAG context found. Using contextual fallback response');
      responseContent = this.generateContextualFallback(lastUserMessage, params.messages);
    }
    
    // Validate response for safety
    const validationResult = this.validator.validateResponse(
      lastUserMessage,
      responseContent,
      ragContext.relevantKnowledge
    );
    
    if (!validationResult.isValid) {
      console.warn('[Web Mock] Response validation failed:', validationResult.warnings);
      responseContent = this.validator.getSafeResponse(lastUserMessage, validationResult);
    }

    // Simulate processing delay
    await new Promise(resolve => setTimeout(resolve, 300));

    return {
      id: `gemma-web-${Date.now()}`,
      object: 'chat.completion',
      created: Math.floor(Date.now() / 1000),
      model: params.model,
      choices: [{
        index: 0,
        message: {
          role: 'assistant',
          content: responseContent
        },
        finish_reason: 'stop'
      }],
      usage: {
        prompt_tokens: ragContext.contextualPrompt.length / 4,
        completion_tokens: responseContent.length / 4,
        total_tokens: (ragContext.contextualPrompt.length + responseContent.length) / 4
      }
    };
  }

  private getFormattedRAGResponse(ragContext: any): string {
    const topKnowledge = ragContext.relevantKnowledge[0];
    let response = topKnowledge.content;

    // Add context if available
    if (topKnowledge.context) {
      response += `\n\n**Context:** ${topKnowledge.context}`;
    }

    // Add source information
    if (topKnowledge.source) {
      response += `\n\n*Source: ${topKnowledge.source}*`;
    }

    return response;
  }

  // å¯¹è¯å†å²æ ¼å¼åŒ–ï¼šæ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
  private formatConversationHistory(messages: Message[]): string {
    if (messages.length === 0) return '';
    
    // åªä¿ç•™æœ€è¿‘6è½®å¯¹è¯ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
    const recentMessages = messages.slice(-12); // 6è½®å¯¹è¯ = 12æ¡æ¶ˆæ¯ï¼ˆç”¨æˆ·+åŠ©æ‰‹ï¼‰
    
    let conversationContext = 'å¯¹è¯å†å²:\n';
    
    for (const msg of recentMessages) {
      if (msg.role === 'user') {
        conversationContext += `ç”¨æˆ·: ${msg.content}\n`;
      } else if (msg.role === 'assistant') {
        conversationContext += `åŠ©æ‰‹: ${msg.content}\n`;
      }
    }
    
    conversationContext += '\nå½“å‰é—®é¢˜: ';
    return conversationContext;
  }

  // ç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„é™çº§å›å¤
  private generateContextualFallback(currentMessage: string, messages: Message[]): string {
    const hasEarthquakeContext = messages.some(msg => 
      msg.content.toLowerCase().includes('åœ°éœ‡') || 
      msg.content.toLowerCase().includes('earthquake')
    );
    
    const hasDisasterContext = messages.some(msg => 
      msg.content.toLowerCase().includes('ç¾éš¾') || 
      msg.content.toLowerCase().includes('emergency') ||
      msg.content.toLowerCase().includes('æ•‘æ´')
    );

    let contextualResponse = '[Web Mock] ';
    
    if (hasEarthquakeContext) {
      contextualResponse += `æˆ‘æ³¨æ„åˆ°æ‚¨ä¹‹å‰è¯¢é—®äº†åœ°éœ‡ç›¸å…³çš„é—®é¢˜ã€‚å…³äº"${currentMessage}"ï¼Œæˆ‘åœ¨åœ°éœ‡çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°å…·ä½“ä¿¡æ¯ã€‚`;
    } else if (hasDisasterContext) {
      contextualResponse += `åŸºäºæˆ‘ä»¬ä¹‹å‰å…³äºç¾éš¾åº”æ€¥çš„å¯¹è¯ï¼Œå…³äº"${currentMessage}"ï¼Œæˆ‘éœ€è¦æ›´å¤šå…·ä½“ä¿¡æ¯æ‰èƒ½æä¾›å‡†ç¡®å»ºè®®ã€‚`;
    } else {
      contextualResponse += `å…³äº"${currentMessage}"ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚`;
    }
    
    contextualResponse += `\n\nğŸ’¡ æç¤ºï¼šåœ¨Webç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä¸»è¦èƒ½å›ç­”åœ°éœ‡å®‰å…¨å’Œç¾éš¾åº”æ€¥ç›¸å…³çš„é—®é¢˜ã€‚\n\nğŸš€ è¦è·å¾—å®Œæ•´çš„AIå¯¹è¯èƒ½åŠ›ï¼Œè¯·åœ¨Androidè®¾å¤‡ä¸Šè¿è¡Œæ­¤åº”ç”¨ã€‚\n\nğŸ” æ‚¨å¯ä»¥å°è¯•é—®æˆ‘ï¼š\n- "åœ°éœ‡æ—¶åº”è¯¥æ€ä¹ˆåŠï¼Ÿ"\n- "å¦‚ä½•å‡†å¤‡åº”æ€¥åŒ…ï¼Ÿ"\n- "é«˜æ¥¼é‡åˆ°åœ°éœ‡æ€ä¹ˆåŠï¼Ÿ"`;
    
    return contextualResponse;
  }
}

// --- OpenAI-compatible interface wrapper ---
export class GemmaOpenAIWrapper {
  private client: GemmaClient;

  constructor(config: { apiKey?: string; dangerouslyAllowBrowser?: boolean } = {}) {
    // Ignore OpenAI-specific config, use our Gemma client
    this.client = new GemmaClient();
  }

  chat = {
    completions: {
      create: (params: ChatCompletionParams): Promise<ChatCompletionResponse> => {
        return this.client.createChatCompletion(params);
      }
    }
  };

  // Placeholder for other OpenAI API endpoints (not implemented)
  images = {
    generate: (params: any) => {
      throw new Error('[Web Mock] Image generation not supported in web version');
    }
  };
}
